# DAY 09 – CV, NLP, Audio
## Оглавление
1. [Глава I](#глава-i) \
    1.1. [Преамбула](#преамбула)
2. [Глава II](#глава-ii) \
    2.1. [Общая инструкция](#общая-инструкция)
3. [Глава III](#глава-iii) \
    3.1. [Цели](#цели)
4. [Глава IV](#глава-iv) \
    4.1. [Задание](#задание)
5. [Глава V](#глава-v) \
    5.1. [Сдача работы и проверка](#сдача-работы-и-проверка)

## Глава I
### Преамбула
Искусственный интеллект - это область компьютерных наук, которая занимается разработкой алгоритмов и программ, 
которые имитируют человеческий интеллект. Существует несколько областей искусственного интеллекта, 
каждая из которых имеет свои собственные методы и задачи.

CV (Computer Vision) - это область искусственного интеллекта, которая занимается разработкой алгоритмов и программ 
для анализа и обработки изображений. Она используется в различных сферах, таких как медицина,
автомобильная промышленность, розничная торговля и многие другие. Задачи CV могут включать в себя распознавание 
объектов на изображении, классификацию изображений, сегментацию изображений и детектирование объектов. Например, 
CV может использоваться для распознавания лиц на фотографиях, определения состояния дорог на основе изображений с камер 
наблюдения или для автоматического анализа рентгеновских снимков.

NLP (Natural Language Processing) - это область искусственного интеллекта, которая занимается разработкой алгоритмов и 
программ для обработки естественного языка. NLP используется в таких областях, как машинный перевод, анализ тональности 
текста, классификация текста и извлечение информации из текста. Например, NLP может использоваться для автоматического 
перевода текстов на другие языки или для анализа тональности отзывов на сайтах.

Audio - это область искусственного интеллекта, которая занимается разработкой алгоритмов и программ для обработки звука.
Она используется в различных областях, таких как медицина, музыкальная индустрия, телекоммуникации и многие другие. 
Задачи Audio могут включать в себя распознавание речи, классификацию звуков, обработку звуковых сигналов и синтез звука. 
Например, Audio может использоваться для распознавания голосовых команд на устройствах умного дома или для классификации 
звуковых сигналов на записях электроэнцефалограмм.

## Глава II
### Общая инструкция

Методология Школы 21 может быть не похожа на тот образовательный опыт, который случался с тобой ранее. Её отличает высокий уровень автономии: у тебя есть задача, ты должен её выполнить. По большей части, тебе нужно будет самому добывать знания для её решения. Второй важный момент — это peer-to-peer обучение. В образовательном процессе нет менторов и экспертов, перед которыми ты защищаешь свой результат. Ты это делаешь перед таким же учащимися, как и ты сам. У них есть чек-лист, который поможет им качественно выполнить приемку вашей работы.

Роль Школы 21 заключается в том, чтобы обеспечить через последовательность заданий и оптимальный уровень поддержки такую траекторию обучения, при которой ты не только освоишь hard skills, но и научишься самообучаться.

- Не доверяй слухам и предположениям о том, как должно быть оформлено ваше решение. Этот документ является единственным источником, к которому стоит обращаться по большинству вопросов;
- твое решение будет оцениваться другими учащимися;
- подлежат оцениванию только те файлы, которые ты выложил в GIT (ветка develop, папка src);
- в твоей папке не должно быть лишних файлов — только те, что были указаны в задании;
- не забывай, что у вас есть доступ к интернету и поисковым системам;
- обсуждение заданий можно вести и в Rocket.Chat;
- будь внимателен к примерам, указанным в этом документе — они могут иметь важные детали, которые не были оговорены другим способом;
- и да пребудет с тобой Сила!



## Глава III
### Цели
В этом командном проекте ты познакомишься с тремя областями искусственного интеллекта: CV, NLP, Audio. \
Каждая из этих областей - это отдельный мир со своими методами, алгоритмами, подходами и задачами. Разделите обязанности 
и задачи внутри команды и выполните все три части проекта. 

## Глава IV
### Computer Vision
В этой части проекта ты познакомишься с компьютерным зрением, со сверточными нейронными сетями 
и их обучением для классификации изображений с использованием библиотеки [PyTorch](https://pytorch.org/). 
Работать мы будем с набором данных [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html). 
CIFAR-10 состоит из 60000 цветных изображений 32x32 в 10 классах, по 6000 изображений в каждом классе. 
Имеется 50000 обучающих изображений и 10000 тестовых изображений.
![cifar10](misc/images/cifar10.png)


#### Task 1.1
Давай загрузим датасет CIFAR-10. Допиши функцию [load_dataloaders](./code-samples/cv_utils.py) с помощью [torchvision.datasets.CIFAR10](https://pytorch.org/vision/main/generated/torchvision.datasets.CIFAR10.html)
и [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader), чтобы функция возвращала
DataLoaderы для train и test частей датасета. \
C помощью функции `len` количество батчей в train_loader и test_loader.
> Для DataLoader параметры `transform` и `batch_size` оставь по умолчанию.

#### Task 1.2
Узнай, как из объекта DataLoader можно получить изображения и метки. \
Передай первые 4 изображения и метки из первого батча тестовой выборки в функцию [imshow](./code-samples/cv_utils.py).
С помощью нее можно визуализировать датасет. \
Должна получиться примерно такая визуализация:
![sample](./misc/images/images_sample.png)
> Картинки и метки могут отличаться. Главное чтобы метки сходились с изображениями)

#### Task 1.3
Теперь попробуем написать небольшую сверточную нейронную сеть, которую мы будем обучать классифицировать изображения.

Напишем сеть, основанную на одном блоке архитектуры [ResNet](https://arxiv.org/pdf/1512.03385.pdf) - Residual-Block. 
Схема этого блока приведена ниже:

<img src="./misc/images/rediual_block.png" width="500"/>

Допиши класс ResidualNet:
- Все сверточные слои должны иметь 32 выходных канала, а также не должны изменять ширину и высоту изображения.
- Также в сверточных слоях `padding = 1`

Функции, которые тебе понадобятся: 
[Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html), [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html), [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html).

Для базовой проверки, что сеть написана верно, этот код не должен выдавать ошибку\
`assert net(torch.zeros((10, 3, 32, 32))).shape == (10, 10)`

#### Task 1.4
Перейдем к обучению сети. В этом поможет класс [Trainer](./code-samples/cv_utils.py).\
Для обучения, кроме самой модели, 
требуется определить оптимизатор и функцию ошибок:
* В качестве оптимизатора выбери [стохастический градиентный спуск](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)
* В качестве функции ошибок
[кросс-энтропия](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)

Обучи сеть и с помощью функции [plot_train_log](./code-samples/cv_utils.py), визуализируй процесс обучения модели.

#### Task 1.5
Одной стандартной техникой, применяющейся в глубинном обучении, а особенно часто в компьютерном зрении, являются аугментации данных. \
Суть аугментаций состоит в том, что мы можем некоторым синтетическим образом видоизменять объекты обучающей выборки,  тем самым расширяя ее, а также делая итоговую модель более устойчивой к таким изменениям.
Наиболее удобным способом работы с аугментациями в PyTorch является их задание в списке `transforms`, который затем передается в Dataloader.\
Изучи, какие [способы аугментаций](https://pytorch.org/vision/main/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py) 
изображений можно использовать в PyTorch. Выбери несколько из них и визуализируй, как изменился датасет. 
![sample](./misc/images/images_sample.png)
![sample](./misc/images/augment.png)

Обучи сеть с аугментацией данных и с помощью функции [plot_train_log](./code-samples/cv_utils.py), визуализируй процесс обучения модели.

### Natural Language Processing
В этой части проекта ты познакомишься с обработкой естественного языка и решишь задачу классификации токсичных комментариев.
Работать мы будем с 160.000 комментариями, которые были помечены оценщиками как токсичное поведение. Различные 
платформы/сайты могут иметь различные стандарты для процесса отбора токсичных комментариев. Поэтому комментарии помечаются
6 категориями: `toxic`, `severe_toxic`, `obscene`, `threat`, `insult`, `identity_hate`.

#### Task 2.1
Загрузи [датасет](https://files.sberdisk.ru/s/HHNjDEgdCEdHk2U). Отрисуй Bar-plot с количеством комментариев для каждой категории токсичности 
и комментариев без токсичности.
![sample](./misc/images/bar-plot.png)
Выведи эти количества для каждой категории. 

#### Task 2.2
Используя библиотеку [nltk](https://www.nltk.org/), обработай тексты комментариев: 
* Приведи текст к нижнему регистру и токенизируй
* Оставь токены, являющимися английскими словами 
* Убери знаки препинания/цифр
* Убери стоп-слова из библиотеки nltk

Выведи количество токенов для комментария с индексом `000103f0d9cfb60f`

#### Task 2.3
Раздели выборку на тренировочную и тестовую с параметрами `random_state=21, test_size=0.3, shuffle=True`
Преобразуй тексты комментариев с помощью [TF-IDF](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) с количеством признаков, равному 3000. \
Выведи размерность тренировочной выборки после преобразования текста.

#### Task 2.4
Реши задачу [Multi-Label](https://scikit-learn.org/stable/modules/multiclass.html) классификации, используя [RidgeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier).
Посчитай значение ROC-AUC для тестовой выборки.

#### Task 2.5
Если посмотреть на тексты токсичных комментариев, то можно заметить, что они могут повторять одни и те же слова. Или там чаще встречается `!` знаки. \
Кроме 'прямых' текстовых признаков, можно использоваться и эту статистику по тексту. Например, долю уникальных слов в тексте комментария или количество пунктуации в тексте.

Придумай не меньше 4-х признаков, которые могут помочь в обучении модели. Добавь их к признакам TF-IDF и обучи модель. 
Получилось ли увеличить качество классификации?

### Audio
В этой части проекта ты познакомишься с анализом аудио. Скачай датасет с [аудио записями](https://files.sberdisk.ru/s/JRXlbcopIuvyFMZ).
Все они были записаны одним и тем же мужчиной, говорящим на иврите. В каждом файле человек произносит 8 слов; 
каждое слово на иврите означает либо "да", либо "нет", поэтому каждый файл представляет собой 
случайную последовательность из 8 "да" или "нет". 
Отдельной расшифровки не предусмотрено; последовательность закодирована в названии файла, например, 1 означает "да", а 0 - "нет"
Давай попробуем реализовать классификатор слов - произносится "да"/"нет".

#### Task 3.1
Для начала познакомимся с этими записями. \
Установи библиотеку [librosa](https://librosa.org/). Это популярная библиотека для работы с аудио.
Визуализируй аудио сигнал файла `0_1_0_1_1_1_0_0.wav` с помощью функции [librosa.display.waveshow](https://librosa.org/doc/main/generated/librosa.display.waveshow.html)
График должен быть такой же, как показано ниже (по значениям):

![waveform](misc/images/waveform.png)
>Для того, чтобы прослушать это аудио файл, можешь воспользоваться [IPython.display.Audio](http://ipython.org/ipython-doc/stable/api/generated/IPython.display.html#IPython.display.Audio).

#### Task 3.2
Для классификации обычно используют не просто аудио сигнал, а его частотно-временное представление. Для этого сигнал требуется
преобразовать с помощью [оконного преобразования Фурье](https://clck.ru/34JnZD).
С помощью функции [librosa.display.specshow](https://librosa.org/doc/main/generated/librosa.display.specshow.html) 
выведи спектрограмму сигнала. \
График должен быть такой же, как показано ниже (по значениям):
![sftp](misc/images/sftp.png)

#### Task 3.3
C помощью функции [load_dataset](code-samples/audio_utils.py) загрузи датасет. \
Раздели его на train и test c параметрами `test_size=0.2`, `random_state=42`. \
Выведи количество файлов в train и test частях.

#### Task 3.4
Наши аудио записи содержат как речь человека, так и молчание. Для каждой записи нам нужно определить сегменты записи, 
где человек молчит, а где произносит слова. \
Эта задача называется [Voice Activity Detection (VAD)](https://ru.wikipedia.org/wiki/Voice_Activity_Detection).
Придумай или найди метод, по которому можно распознавать участки с речью на аудио записи.

Например:
Запись '0_0_0_1_0_1_1_0.wav' содержит 137592 отсчетов. Сегменты с речью для этой записи (Отмечены красным):
[[23996, 32539],
 [35410, 44925],
 [49493, 57410],
 [60458, 68635],
 [73308, 81278],
 [84001, 91942],
 [97381, 104166],
 [109018, 115573]] 
![sftp](misc/images/vad.png)

Выведи несколько примеров работы твоего VAD-алгоритма, по аналогии с примером, для других аудио записей. Попробуй добиться
наилучшего качества нахождения речи.

#### Task 3.5
После того как мы узнали сегменты аудио с речью, то можно перейти к самой задаче классификации. \
Внимательно изучи функцию [make_dataset](code-samples/audio_utils.py). С помощью этой функции cгенерируй X, Y для train и test выборок.
Затем попробуй обучить различные классификаторы. Например, SVM или LogisticRegression.
Измерь точность (accuracy) классификации на тестовой выборке.

## Глава V
### Сдача работы и проверка

1. Сохрани решения в файлах 09-audio.ipynb, 09-cv.ipynb, 09-nlp.ipynb соответственно.
2. Загрузи изменения на Git в ветку develop.

💡 [Нажми здесь](https://forms.gle/NFqiRmZNXTqpaYQX6) **чтобы отправить обратную связь по проекту**.
